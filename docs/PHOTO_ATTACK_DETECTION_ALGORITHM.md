# 照片攻击检测算法文档

## 概述

本文档详细介绍了两种照片攻击检测方案的原理、实现和使用方法。这两种方案可以独立使用，也可以结合使用以提高检测准确率。

**两个方案的优先级：** 几何约束约束（方案二）> 深度方差分析（方案一）
- 因为几何约束是物理定律，无法被欺骗
- 深度方差依赖于 MediaPipe 的推断，可能有误差

---

## 方案一：MediaPipe 3D 关键点深度方差分析

### 核心原理

MediaPipe 可以从单张 2D 图像推断出 468 个人脸关键点的 3D 坐标 `(x, y, z)`，其中：
- `x, y`：2D 投影坐标（图像平面）
- `z`：相对深度（以鼻尖为参考原点，单位为归一化尺度）

**关键洞察：**

| 特征 | 真实人脸 | 照片 |
|------|---------|------|
| Z 坐标来源 | 真实 3D 结构 | 平面图像（2D） |
| Z 坐标方差 | 大（0.001 ~ 0.01） | 小（< 0.0001） |
| 深度分布 | 鼻子最突出，耳朵靠后 | 所有点共面 |
| 多帧稳定性 | 保持一致 | 完全一致 |

### 检测步骤

1. **帧收集**：收集 N 帧（建议 N ≥ 5）
   ```
   每帧人脸必须：
   - 被 MediaPipe 正确检测
   - 占图像的 30% ~ 90%
   - 已获得完整的 468 个关键点
   ```

2. **关键点选择**：选择有深度差异的特征点
   ```
   典型特征点：
   - 点 1:   鼻尖（最突出）
   - 点 234: 左耳屏（靠后）
   - 点 454: 右耳屏（靠后）
   - 点 152: 下巴
   - 点 10:  额头
   - 点 175: 上唇凹（中等深度）
   ```

3. **计算每帧的深度方差**：
   ```
   对帧 i：
     zValues = [z_{1}, z_{234}, z_{454}, z_{152}, z_{10}, z_{175}]
     depth_var[i] = var(zValues)
   ```

4. **计算多帧平均深度方差**：
   ```
   avg_depth_var = mean(depth_var[1..N])
   ```

5. **判决**：
   ```
   if avg_depth_var > 1e-4:
     --> 真实人脸（深度充分）
   else if avg_depth_var < 1e-5:
     --> 照片（深度极少）
   else:
     --> 介于两者，线性插值判定
   ```

### 阈值标定

通过实际测试，得到以下经验阈值：

| 场景 | 深度方差范围 | 判定 |
|------|-----------|------|
| 正常活体 | 1e-3 ~ 1e-2 | ✅ 真实人脸 |
| 正常活体 | 1e-4 ~ 1e-3 | ✅ 真实人脸（弱信号） |
| 边界值 | 1e-5 ~ 1e-4 | ⚠️ 不确定 |
| 屏幕照片 | < 1e-5 | ❌ 照片 |
| 打印照片 | < 1e-5 | ❌ 照片 |

### 优点

✅ **完全不依赖背景**
- 对白墙、黑墙、复杂背景均有效
- 不需要分离前景/背景

✅ **只依赖人脸本身的 3D 结构**
- 不受光照、角度、距离等太大影响
- 稳定性高

✅ **检测逻辑简单清晰**
- 基于人脸的几何特性
- 无需复杂的运动分析

### 缺点

❌ **依赖 MediaPipe 的推断质量**
- MediaPipe 的 3D 坐标本质上是从 2D 图像推断的
- 极端角度或低分辨率下推断可能失准

❌ **需要足够的帧数**
- 推荐至少 5 帧以上
- 可能需要 0.3~0.5 秒的视频片段

❌ **对快速变化的表情敏感**
- 虽然选了稳定的特征点，但仍可能受大幅表情影响

---

## 方案二：关键点运动透视一致性检验

### 核心原理

**透视效应的核心差异：**

```
真实人脸（3D）旋转：
├─ 近处点（如鼻尖）移动幅度大
├─ 中距离点（脸颊）移动幅度中等
└─ 远处点（耳朵）移动幅度小
   → 各点位移【差异大】

平面照片（2D）旋转：
├─ 所有点遵循同一仿射变换
├─ 各点位移【完全一致】
└─ 透视比率 ≈ 1.0（近点 ÷ 远点）
   → 无法区分远近，所以位移一致
```

### 检测步骤

#### 步骤 1：帧数据准备
收集至少 3 帧连续的人脸检测结果，每帧包含 468 个 2D 归一化坐标 `(x, y)`

#### 步骤 2：选择关键特征点
```typescript
const keyPointIndices = {
  near: [1, 4, 6, 195],    // 鼻子及周围（近处点）
  mid: [127, 356],          // 脸颊（中距离点）
  far: [162, 389]           // 耳朵（远处点）
}
```

#### 步骤 3：计算帧间位移
```typescript
for frame[i] to frame[i+1]:
  for each point in keyPointIndices:
    displacement = {
      x: frame[i+1].x - frame[i].x,
      y: frame[i+1].y - frame[i].y,
      magnitude: sqrt(x² + y²)
    }
```

#### 步骤 4：计算 4 个关键指标

##### 指标 A：运动位移方差
```
motionDisplacementVariance = var([magnitude_near, magnitude_mid, magnitude_far])

解释：
- 真人：各点位移差异大 → 方差高（0.02+）
- 照片：各点位移一致 → 方差低（< 0.01）
```

##### 指标 B：透视比率
```
perspectiveRatio = avg(magnitude_near) / avg(magnitude_far)

解释：
- 真人：近点移动幅度 > 远点 → 比率 > 1.0
- 照片：近点 = 远点 → 比率 ≈ 1.0
```

##### 指标 C：运动方向一致性
```
对每个点的运动向量计算：
  方向向量 = displacement / |displacement|
  
平均方向 = arctan2(mean(sin), mean(cos))

然后计算每个点与平均方向的夹角余弦值：
  consistency = (cos_angle + 1) / 2  // 转换为 [0, 1]

解释：
- 真人：各点方向差异大 → 一致性低（< 0.5）
- 照片：所有点同向 → 一致性高（> 0.8）
```

##### 指标 D：仿射变换模式匹配
```
仿射变换：p' = A * p + t （矩阵乘法）

对于照片平面旋转，应该满足单一的仿射变换。
尝试用最小二乘法拟合，计算拟合误差：

error = sum((p'_i - fit_p'_i)² for all i)

解释：
- 真人：拟合误差大 → 不符合单一变换
- 照片：拟合误差小 → 完全符合单一变换
```

#### 步骤 5：综合判定

```typescript
// 将 4 个指标归一化为 [0, 1] 的照片置信度
depth_indicator = 1 - (motionDisplacementVariance / threshold)
ratio_indicator = 1 - |perspectiveRatio - 1|
consistency_indicator = motionDirectionConsistency
affine_indicator = affineTransformPatternMatch

// 加权平均（各指标权重相等）
perspectiveScore = (
  depth_indicator + 
  ratio_indicator + 
  consistency_indicator + 
  affine_indicator
) / 4

if perspectiveScore > 0.6:
  --> 检测到照片
else:
  --> 真实人脸
```

### 阈值标定

| 指标 | 真人范围 | 照片范围 | 判定阈值 |
|------|---------|---------|---------|
| 运动方差 | > 0.02 | < 0.01 | 0.015 |
| 透视比率 | > 0.95 | < 0.85 | 0.90 |
| 方向一致性 | < 0.5 | > 0.8 | 0.65 |
| 仿射匹配 | < 0.3 | > 0.8 | 0.55 |

### 优点

✅ **基于纯 2D 几何**
- 不依赖 3D 推断
- 物理定律层面的检测

✅ **对倾角拍摄特别有效**
- 照片从倾角拍摄时仍然遵循透视规律
- 这种规律性成为明显的"照片特征"

✅ **更加稳健**
- 对光照、分辨率变化不敏感
- 基于几何关系，不易被欺骗

### 缺点

❌ **需要足够的运动**
- 如果用户完全静止，无法进行检测
- 需要头部或脸部有运动

❌ **对轻微运动敏感度可能偏低**
- 用户只做极小幅度动作时，可能检测不到

❌ **计算复杂度相对较高**
- 涉及矩阵运算、相关性计算等

---

## 综合判定策略

### 优先级设置

```
优先级 1（最高）：强几何约束信号
  ├─ perspectiveScore > 0.85
  └─ --> 直接拒绝（照片）

优先级 2（次高）：深度方差分析
  ├─ depthVarianceScore > 0.8
  └─ --> 拒绝（照片特征明显）

优先级 3（正常）：综合判定
  ├─ 两个方案的加权平均
  └─ --> 根据置信度决策
```

### 置信度合并

```typescript
// 如果任何一个方案强烈指向"照片"，就判定为照片
photoConfidence = max(
  depthVarianceScore,
  perspectiveScore
)

// 进一步加权：考虑两者都有强信号
if depthVarianceScore > 0.7 AND perspectiveScore > 0.7:
  photoConfidence = min(1.0, photoConfidence + 0.2)
```

### 决策树

```
开始 
  │
  ├─ 深度方差检测
  │   ├─ 高置信度照片特征？ ──YES──> ❌ 拒绝
  │   └─ 不是
  │
  ├─ 透视一致性检验
  │   ├─ 强几何约束信号？ ──YES──> ❌ 拒绝
  │   └─ 不是
  │
  ├─ 综合判定
  │   ├─ 两个方案都指向照片？ ──YES──> ❌ 拒绝
  │   ├─ 一个方案置信度很高？ ──YES──> ❌ 拒绝
  │   └─ 都不是强信号 ──YES──> ✅ 接受
  │
  └─ 结束
```

---

## 实现细节

### 使用 MediaPipe 的 Z 坐标

MediaPipe 返回的 Z 坐标虽然是推断值，但对区分平面照片仍然有意义：

```typescript
// 从 FaceResult 提取 3D 坐标
const mesh3D: Point[] = faceResult.mesh          // 原始坐标
const meshRaw3D: Point[] = faceResult.meshRaw    // 归一化坐标

// Point = [x, y, z]
// 深度方差分析使用 meshRaw（因为已经归一化）
// 透视一致性检验使用 meshRaw（使用 x, y 分量）
```

### 归一化坐标 vs 原始坐标

| 类型 | 坐标范围 | 用途 | 优点 |
|------|---------|------|------|
| 原始 | 像素坐标 | 3D 深度分析、交叉比率 | 物理意义清晰 |
| 归一化 | [0, 1] | 2D 运动分析、相对位移 | 不受人脸大小影响 |

---

## 性能考虑

### 计算复杂度

| 操作 | 时间复杂度 | 备注 |
|------|----------|------|
| 深度方差 | O(N×468) | N=帧数 |
| 位移计算 | O(N×K) | K=关键点数 |
| 相关性计算 | O(N×K²) | 最昂贵的操作 |
| **总计** | **O(N×K²)** | ~15帧×10点² ≈ 1500 次运算 |

### 内存使用

```
帧缓冲：15 帧 × 468 点 × 3 坐标 = 21,060 个浮点数 ≈ 82 KB
临时数据：< 50 KB
总计：~150 KB（可忽略）
```

### 推荐参数

| 参数 | 值 | 原因 |
|------|---|------|
| 帧缓冲大小 | 15 | 0.5 秒@30fps，平衡精度和延迟 |
| 最小帧数 | 5 | 足够计算统计特性，避免噪声 |
| 检测频率 | 每 3 帧 | 实时响应，避免过频繁检测 |

---

## 应对攻击方式

### 防护 1：对抗"倾角照片"

**攻击方式：** 用户拿着一张倾角的照片

**防护原理：**
- 即使倾角，照片仍然是平面的
- 方案二的透视一致性检验可以检测到：虽然有透视变形，但仍遵循单一变换规律
- 真人的倾斜会导致各点运动差异（因为不同部位深度不同）

### 防护 2：对抗"视频回放"

**攻击方式：** 用户用手机播放他人的视频

**防护原理：**
- 视频的每一帧都是 2D 图像，深度方差为零
- 方案一可以直接检测
- 即使视频有动作，也会表现出照片的几何特性

### 防护 3：对抗"3D 打印面具"

**攻击方式：** 用户用 3D 打印的人脸面具（具有真实 3D 结构）

**当前防护：**
- 3D 面具虽然有深度，但会缺少微妙的肌肉动作
- 运动不会表现出真实人脸的透视特性
- 需要结合活体检测（眨眼等）进行防护

---

## 故障排除

### 问题：深度方差始终很大（假阳性）

**原因：** 可能是 MediaPipe 的推断错误

**解决方案：**
```typescript
// 1. 检查图像质量
if (imageQuality < 0.6) {
  console.warn('图像质量过低，可能导致推断失误')
}

// 2. 使用方案二作为验证
if (perspectiveScore < 0.5) {
  console.log('方案一结果不可靠，相信方案二')
}

// 3. 增加缓冲帧数
photoDetector.config.frameBufferSize = 30  // 1 秒数据
```

### 问题：运动一致性无法计算（人脸太静止）

**原因：** 用户完全不动

**解决方案：**
```typescript
if (photoDetector.getFrameCount() >= 5) {
  const result = photoDetector.detect()
  if (result.details.motionDisplacementVariance < 0.001) {
    console.warn('检测到人脸不动，请求用户做出头部运动')
  }
}
```

---

## 参考资源

- [MediaPipe Face Landmarks](https://google.github.io/mediapipe/solutions/face_detection.html)
- [射影几何与交叉比率](https://en.wikipedia.org/wiki/Cross-ratio)
- [仿射变换](https://en.wikipedia.org/wiki/Affine_transformation)
- [人脸活体检测综述](https://arxiv.org/pdf/1805.08937.pdf)
